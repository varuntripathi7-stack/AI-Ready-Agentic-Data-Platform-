#!/usr/bin/env python3
"""
Feature Engineering Pipeline
Creates ML features from Silver layer data for model training.
Features:
  - Purchases in last 24 hours (per user)
  - Average order value (per user)
  - Event frequency (per user)
"""

import os
# Ensure compatible Java version for Spark
if os.path.isdir("/usr/lib/jvm/java-17-openjdk-amd64"):
    os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-17-openjdk-amd64"





# Import SparkSession to create a Spark session for processing data. 

from pyspark.sql import SparkSession


# Import various functions from pyspark.sql.functions to perform data transformations and aggregations.
# These functions include:
# - col: to reference columns in DataFrame operations
# - count: to count occurrences of events
# - sum: to calculate total revenue
# - avg: to calculate average order value
# - max and min: to find maximum and minimum purchase values
# - when: to create conditional columns
# - datediff: to calculate differences between timestamps
# current_timestamp: to get the current timestamp for time-based calculations
# lit: to create literal values in DataFrame operations
# - round: to round numerical values
# - countDistinct: to count unique products interacted with
# - expr: to use SQL expressions in DataFrame operations        
from pyspark.sql.functions import (
    col, count, sum as spark_sum, avg, max as spark_max, min as spark_min,
    when, datediff, current_timestamp, lit, round as spark_round,
    countDistinct, expr, to_timestamp, unix_timestamp
)



# # Import Window for window functions to define partitioning and ordering ((not used in current features but can be useful for future enhancements).)
# for applying SQL-style window functions (e.g., row_number, lag, rank) in PySpark.
from pyspark.sql.window import Window


# Import sys for handling system-level operations, such as exiting the program in case of errors.
import sys

# Import datetime and timedelta for handling date and time operations, 
# such as calculating time differences for features like purchases in the last 24 hours.
from datetime import datetime, timedelta





# Configuration 
# Define paths for Silver data and output features. 
# These paths are constructed based on the directory structure of the project, 
# allowing for easy access to the necessary data and storage of the generated features.
BASE_PATH = os.path.dirname(os.path.dirname(os.path.abspath(__file__))) # Get the base directory of the project
SILVER_PATH = os.path.join(BASE_PATH, "data/silver/ecommerce_events") # Path to the Silver Delta table containing raw event data
FEATURES_PATH = os.path.join(BASE_PATH, "data/features/user_features") # Path where the generated features will be stored as a Delta table





# This Function to create and configure a Spark session with Delta Lake support.
def create_spark_session() -> SparkSession: 
    """
    Create and configure Spark session with Delta Lake support.

    This function:
    Starts Spark
    Configures it
    Enables Delta Lake
    Returns the Spark session
    """
    spark = SparkSession.builder \
        .appName("Feature_Engineering") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.jars.packages", "io.delta:delta-spark_2.12:3.1.0") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    return spark




# Function to read data from the Silver Delta table.
def read_silver_data(spark: SparkSession): 
    """
    Read data from Silver Delta table.

    This function:
    Reads cleaned data from the Silver layer (Delta table)
    Checks if it was read successfully
    Returns the DataFrame
    Stops the program if something fails

    Think of it like: Go to storage, bring cleaned data, and give it to the pipeline.
    """
    print(f"Reading from Silver path: {SILVER_PATH}")
    
    try:
        silver_df = spark.read.format("delta").load(SILVER_PATH)
        record_count = silver_df.count()
        print(f"✓ Read {record_count} records from Silver layer")
        return silver_df
    except Exception as e:
        print(f"✗ Failed to read Silver data: {e}")
        sys.exit(1)






# This Function calculate purchase-related features per user.
def calculate_purchase_features(df, spark: SparkSession): 
    """
    Calculate purchase-related features per user.
    - purchases_last_24h: Number of purchases in last 24 hours
    - total_purchases: Total number of purchases
    - total_revenue: Total revenue generated by user
    - avg_order_value: Average order value
    """
    print("\nCalculating purchase features...")
    
    # Get current timestamp for 24h window calculation
    # Get the current timestamp to calculate features based on recent activity 
    # (e.g., purchases in the last 24 hours).
    current_ts = current_timestamp() 
    
    # Filter for purchase events
    purchases_df = df.filter(col("event_type") == "purchase")
    
    # Calculate purchase features per user
    # Group by user_id and aggregate purchase-related metrics, including:
    # - purchases in the last 24 hours
    # - total purchases
    # - total revenue
    # - average order value
    # - maximum and minimum purchase values
    purchase_features = purchases_df \
        .groupBy("user_id") \
        .agg(
            # Purchases in last 24 hours
            count(
                when(
                    col("event_timestamp") >= current_ts - expr("INTERVAL 24 HOURS"),
                    1
                )
            ).alias("purchases_last_24h"),
            # Total purchases
            count("*").alias("total_purchases"), # It counts all rows in the group, which corresponds to the total number of purchase events for each user.
            # Total revenue
            spark_round(spark_sum("price"), 2).alias("total_revenue"),
            # Average order value
            spark_round(avg("price"), 2).alias("avg_order_value"),
            # Max purchase value
            spark_round(spark_max("price"), 2).alias("max_purchase_value"),
            # Min purchase value
            spark_round(spark_min("price"), 2).alias("min_purchase_value")
        )
    
    return purchase_features




# This Function calculates event frequency features per user, 
# including:-  
# total events, 
# view and cart counts, 
# unique products interacted with, activity span in hours, and events per hour.
def calculate_event_frequency_features(df, spark: SparkSession): 
    """
    Calculate event frequency features per user.
    - total_events: Total number of all events
    - view_count: Number of view events
    - cart_count: Number of cart events
    - event_frequency_per_hour: Average events per hour
    """
    print("Calculating event frequency features...")
    
    # Calculate event counts per user
    event_features = df \
        .groupBy("user_id") \
        .agg(
            # Total events
            count("*").alias("total_events"),
            # View count
            count(when(col("event_type") == "view", 1)).alias("view_count"),
            # Cart count
            count(when(col("event_type") == "cart", 1)).alias("cart_count"),
            # Purchase count
            count(when(col("event_type") == "purchase", 1)).alias("event_purchase_count"),
            # Unique products interacted
            countDistinct("product_id").alias("unique_products"),
            # First and last activity
            spark_min("event_timestamp").alias("first_activity"),
            spark_max("event_timestamp").alias("last_activity")
        ) \
        .withColumn(
            # Time span in hours
            "activity_span_hours",
            spark_round(
                (unix_timestamp(col("last_activity")) - unix_timestamp(col("first_activity"))) / 3600,
                2
            ) # Convert seconds to hours and round to 2 decimals  
              # unix_timestamp() converts a timestamp (date + time) into a number of seconds since: January 1, 1970 (00:00:00 UTC)
        ) \
        .withColumn(
            # Events per hour (avoid divide by zero)
            "events_per_hour",
            spark_round(
                when(col("activity_span_hours") > 0, 
                     col("total_events") / col("activity_span_hours"))
                .otherwise(col("total_events")),
                2
            ) # Calculate events per hour by dividing total events by activity span in hours, and round to 2 decimals. 
              # If activity span is zero, use total events as the value (to avoid division by zero).
        )
    
    return event_features





# This Function calculates conversion-related features per user, 
# including:-
# view_to_cart_ratio: Cart adds / Views
# cart_to_purchase_ratio: Purchases / Cart adds
# overall_conversion: Purchases / Views
def calculate_conversion_features(df, spark: SparkSession):
    """
    Calculate conversion-related features per user.
    - view_to_cart_ratio: Cart adds / Views
    - cart_to_purchase_ratio: Purchases / Cart adds
    - overall_conversion: Purchases / Views
    """
    print("Calculating conversion features...")
    
    conversion_features = df \
        .groupBy("user_id") \
        .agg(
            count(when(col("event_type") == "view", 1)).alias("views"), # Count the number of view events for each user and alias it as "views".
            count(when(col("event_type") == "cart", 1)).alias("carts"), # Count the number of cart events for each user and alias it as "carts".
            count(when(col("event_type") == "purchase", 1)).alias("purchases") # Count the number of purchase events for each user and alias it as "purchases".
        ) \
        .withColumn(
            "view_to_cart_ratio",
            spark_round(
                when(col("views") > 0, col("carts") / col("views")) # Calculate the ratio of cart adds to views for each user, and round to 4 decimals.
                .otherwise(0.0), # If the number of views is zero, set the ratio to 0.0 to avoid division by zero.
                4
            )
        ) \
        .withColumn(
            "cart_to_purchase_ratio",
            spark_round(
                when(col("carts") > 0, col("purchases") / col("carts")) # Calculate the ratio of purchases to cart adds for each user, and round to 4 decimals.
                .otherwise(0.0), # If the number of cart adds is zero, set the ratio to 0.0 to avoid division by zero.
                4
            )
        ) \
        .withColumn(
            "overall_conversion",
            spark_round(
                when(col("views") > 0, col("purchases") / col("views")) # Calculate the overall conversion rate (purchases to views) for each user, and round to 4 decimals.
                .otherwise(0.0), # If the number of views is zero, set the overall conversion to 0.0 to avoid division by zero.
                4
            )
        ) \
        .select(
            "user_id",
            "view_to_cart_ratio",
            "cart_to_purchase_ratio",
            "overall_conversion"
        ) # Select only the relevant columns for conversion features to be used in the final feature set.
    
    return conversion_features







# This Function combines all feature sets (purchase, event frequency, conversion) into a single feature table.
# It also creates a binary target variable "is_purchaser" based on whether the user has made any purchases.
# The combined feature table includes all relevant features and metadata for each user, ready for model training.
# The function also handles users who may not have purchase data by filling nulls with zeros, ensuring that all users are included in the final feature set.
# Finally, it adds a timestamp to indicate when the features were generated.
# Think of it like: We have different pieces of information about users (their purchases, how often they interact, and how well they convert).
# We want to bring all that information together into one big table that we can use to train our model to predict who is likely to make a purchase.
def combine_features(purchase_features, event_features, conversion_features):
    """
    Combine all feature sets into a single feature table.
    - Join purchase, event frequency, and conversion features on user_id
    - Fill nulls for users without purchases
    - Create target variable: is_purchaser (1 if total_purchases > 0
        else 0)
         - Add feature timestamp
         - Select final feature columns
         - Return combined feature DataFrame

    """
    print("\nCombining all features...")
    
    # Start with event features (all users)
    combined = event_features \
        .join(purchase_features, on="user_id", how="left") \
        .join(conversion_features, on="user_id", how="left")
    
    # Fill nulls for users without purchases
    combined = combined \
        .fillna(0, subset=[
            "purchases_last_24h", "total_purchases", "total_revenue",
            "avg_order_value", "max_purchase_value", "min_purchase_value"
        ])
    
    # Add feature timestamp
    combined = combined.withColumn("feature_timestamp", current_timestamp())
    
    # Create target variable: is_purchaser (binary classification target)
    combined = combined.withColumn(
        "is_purchaser",
        when(col("total_purchases") > 0, 1).otherwise(0)
    )
    
    # Select final feature columns
    final_features = combined.select(
        "user_id",
        # Purchase features
        "purchases_last_24h",
        "total_purchases",
        "total_revenue",
        "avg_order_value",
        "max_purchase_value",
        "min_purchase_value",
        # Event frequency features
        "total_events",
        "view_count",
        "cart_count",
        "unique_products",
        "activity_span_hours",
        "events_per_hour",
        # Conversion features
        "view_to_cart_ratio",
        "cart_to_purchase_ratio",
        "overall_conversion",
        # Metadata
        "first_activity",
        "last_activity",
        "feature_timestamp",
        # Target
        "is_purchaser"
    )
    
    return final_features



# This Function writes the final feature table to a Delta Lake location specified by output_path.
def write_features(df, output_path: str):
    """
    Write feature table to Delta Lake.
    - Write the final feature DataFrame to the specified output path in Delta format.
        - Use overwrite mode to replace existing data
        - Enable overwriteSchema to allow schema changes
        - Print the number of records written for verification
    """
    print(f"\nWriting features to: {output_path}")
    
    df.write \
        .format("delta") \
        .mode("overwrite") \
        .option("overwriteSchema", "true") \
        .save(output_path)
    
    record_count = df.count() # Count the number of records in the DataFrame to confirm how many feature records were written to the Delta Lake.
    print(f"✓ Wrote {record_count} user feature records") 




# Main function to run the feature engineering pipeline.
def main():
    """
    Main function to run the feature engineering pipeline.
    """
    print("=" * 60)
    print("Feature Engineering Pipeline")
    print("=" * 60)
    print(f"Silver Path: {SILVER_PATH}")
    print(f"Features Path: {FEATURES_PATH}")
    print("=" * 60)
    
    # Create Spark session
    spark = create_spark_session()
    print("✓ Spark session created")
    
    # Read Silver data
    silver_df = read_silver_data(spark)
    
    # Cache for multiple passes
    silver_df.cache()
    
    # Calculate feature groups
    purchase_features = calculate_purchase_features(silver_df, spark)
    event_features = calculate_event_frequency_features(silver_df, spark)
    conversion_features = calculate_conversion_features(silver_df, spark)
    
    # Combine all features
    final_features = combine_features(purchase_features, event_features, conversion_features)
    
    # Show sample
    print("\nSample Feature Records:")
    final_features.select(
        "user_id", "total_events", "total_purchases", 
        "avg_order_value", "events_per_hour", "is_purchaser"
    ).show(10)
    
    # Write to Delta
    write_features(final_features, FEATURES_PATH)
    
    # Unpersist
    silver_df.unpersist()
    
    print("\n" + "=" * 60)
    print("Feature Engineering Complete")
    print("=" * 60)
    
    spark.stop()


if __name__ == "__main__":
    main()
